// This autogenerated skeleton file illustrates how to build a server.
// You should copy it to another filename to avoid overwriting it.

#include "MR_ServerInterface.h"
#include "../gen-cpp-node/MR_ComputeNodeInterface.h"
#include <thrift/protocol/TBinaryProtocol.h>
#include <thrift/server/TSimpleServer.h>
#include <thrift/server/TThreadedServer.h>
#include <thrift/transport/TServerSocket.h>
#include <thrift/transport/TBufferTransports.h>
#include <thrift/transport/TSocket.h>

#include <sys/time.h>
#include <fstream>
#include <sstream>
#include <iostream>
#include <istream>
#include <ifaddrs.h>
#include <netinet/in.h>
#include <string.h>
#include <arpa/inet.h>
#include <pthread.h>

using namespace ::apache::thrift;
using namespace ::apache::thrift::protocol;
using namespace ::apache::thrift::transport;
using namespace ::apache::thrift::server;

using boost::shared_ptr;

#define HEARTBEAT_TIMEOUT_MS 2000

enum taskType{SORT, MERGE};

typedef struct Task{
  bool completed;
  taskType type;
  int offset;
  int size;
  int merge_task_number;
  std::vector<std::string> filenames;
}Task;

struct Info{
  std::vector<Task> tasks;
  bool dead;
  int64_t heartbeat;
};

struct ServerInfo{
  std::string ipAddress;
  int16_t port;
};

//this is needed to use the map data structure
bool operator <(const ServerInfo& lhs, const ServerInfo& rhs)
{
  return lhs.ipAddress < rhs.ipAddress || lhs.port < rhs.port;
}

bool operator ==(const ServerInfo& lhs, const ServerInfo& rhs)
{
  return lhs.ipAddress == rhs.ipAddress && lhs.port == rhs.port;
}

bool operator ==(const Task& lhs, const Task& rhs)
{
  if(&lhs == &rhs)
    return true;
  return false;
}
pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t merge_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t sort_lock = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t q = PTHREAD_MUTEX_INITIALIZER;
pthread_mutex_t num_r_tasks_lock = PTHREAD_MUTEX_INITIALIZER;
void *ServerHandle;

void *heartbeatTimerWrapper(void*);

class MR_ServerInterfaceHandler : virtual public MR_ServerInterfaceIf {
 private:
   std::map<ServerInfo, Info> allServer;
   std::vector<std::string> filenames;
   std::vector<std::string> tempFilenames;
   std::map<ServerInfo, Info>::iterator merge_iter;

   int max_size;
   unsigned int merge_inter_files,
                merge_files_left, //Number of files left to merge
                merge_task_number, //Task number used to name the intermediary files
                merge_expected_files, //Number of files that are expected to be produced
                request_number, //Number of requests processed
                num_sort_files,
                total_files,
                redundancy,
                total_alive_nodes,
                num_faults,
                num_redundant_tasks,
                num_redundant_tasks_killed;

 public:
  MR_ServerInterfaceHandler(int chunk_size, int merge_num_files, int redundant_tasks) {
    // Your initialization goes here
    max_size = chunk_size;
    merge_inter_files = merge_num_files;
    merge_files_left = 0;
    merge_task_number = 0;
    merge_expected_files = 0;
    request_number = 0;
    num_sort_files = 0;
    total_files = 0;
    redundancy = redundant_tasks;
    total_alive_nodes = 0;
    num_faults = 0;
    num_redundant_tasks = 0;
    num_redundant_tasks_killed = 0;
    merge_iter = allServer.begin();

    //start heartbeat timer in background.
    ServerHandle = (void*)this;
    pthread_t Thread;
    pthread_create(&Thread, NULL, heartbeatTimerWrapper, NULL);
    pthread_detach(Thread);
  }

  int64_t Register(const std::string& ipAddress, const int16_t port) {
    ServerInfo temp;
    pthread_mutex_lock(&m);
    temp.port = port;
    temp.ipAddress = ipAddress;

    std::cout << "Node with IP: " << temp.ipAddress << " and port: " << temp.port << " has registered."<< std::endl;

    Info temp2;
    temp2.dead = false;
    struct timeval tv1;
    gettimeofday(&tv1, NULL); //set heartbeat to current time
    temp2.heartbeat = tv1.tv_sec * 1000000 + tv1.tv_usec;

    allServer.insert(std::pair<ServerInfo, Info>(temp, temp2));
    std::cout << allServer.size() << std::endl;
    total_alive_nodes++;
    pthread_mutex_unlock(&m);
    return 1;
  }

  void sendHeartbeat(const std::string& ipAddress, const int16_t port) {
    ServerInfo node;
    node.ipAddress = ipAddress;
    node.port = port;
    std::stringstream ss;
    ss << port;
    struct timeval tv1;
    gettimeofday(&tv1, NULL); //set heartbeat to current time

    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++) {
          if(iter->first == node) {
            iter->second.heartbeat = tv1.tv_sec * 1000000 + tv1.tv_usec;
        }
    }
  }

  void heartbeatTimer() {
    while(true)
    {
      for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
          iter != allServer.end(); iter++)
      {
        struct timeval tv1;
        gettimeofday(&tv1, NULL); //set heartbeat to current time
        int64_t now = tv1.tv_sec * 1000000 + tv1.tv_usec;
        //if heartbeat has expired, set node to dead and reassign tasks
        if(!iter->second.dead && (iter->second.heartbeat + HEARTBEAT_TIMEOUT_MS * 1000 < now))
        {
          std::cout << "Node " << iter->first.ipAddress << ":" << iter->first.port
                    << " died :(  Reassigning workload." << std::endl;
          iter->second.dead = true;
          total_alive_nodes--;
          num_faults++;
          std::cout << "# of Alive Compute Nodes: " << total_alive_nodes << std::endl;

          //all tasks are either sort tasks or merge tasks
          if(iter->second.tasks.size() > 0)
          {
            if(iter->second.tasks[0].type == SORT)
              reassignSortTasks(iter->second.tasks, 1);
            else if(iter->second.tasks[0].type == MERGE){
              reassignMergeTasks(iter->second.tasks, 1);
            }
          }

          try {
            shared_ptr<TSocket> socket(new TSocket(iter->first.ipAddress, iter->first.port));
            shared_ptr<TTransport> transport(new TBufferedTransport(socket));
            shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
            MR_ComputeNodeInterfaceClient ComputerNode(protocol);
            //connect to node and call sort
            transport->open();
            ComputerNode.pingable();
            transport->close();
          } catch (...) {
            //Remove from allServers
            std::cout << "Node has been terminated. Unregistering node." <<std::endl;
            allServer.erase(iter);
          }
        }
      }
      usleep(200000); //run every 100ms
    }
  }

  void reassignSortTasks(std::vector<Task> &tasks, int redundancy) {
    unsigned int count = 0;
    std::vector<Task>::iterator task_iter;
    bool task_exists = false;
    for(int i = 0; i < redundancy; i++)
    {
      task_iter = tasks.begin();
      int offset;
      int size;
      std::map<ServerInfo, Info>::iterator iter = allServer.begin();
      while(task_iter != tasks.end())
      {
        //don't reassign a completed task
        while((task_iter != tasks.end()) && task_iter->completed)
          task_iter++;
        if(task_iter == tasks.end()) //All tasks are completed
          break;

        task_exists = false;
        for(unsigned int j = 0; j < iter->second.tasks.size(); j++)
        {
          if(task_iter->offset == iter->second.tasks[j].offset &&
              task_iter->size == iter->second.tasks[j].size)
          {
            task_exists = true;
            break;
          }
        }
        if(task_exists)
        {
          count++;
          if(count > total_alive_nodes)
          {
            count = 0;
            task_iter++;
          }
          else
          {
            iter++;
            if(iter == allServer.end())
              iter = allServer.begin();
          }
          continue;
        }
        //find a free task that isn't performing this task
        if(!iter->second.dead)
        {
          size = task_iter->size;
          offset = task_iter->offset;

          try {
            Task temp;
            temp.type = SORT;
            temp.offset = offset;
            temp.size = size;
            temp.filenames = task_iter->filenames;
            temp.completed = false;
            iter->second.tasks.push_back(temp);

            shared_ptr<TSocket> socket(new TSocket(iter->first.ipAddress, iter->first.port));
            shared_ptr<TTransport> transport(new TBufferedTransport(socket));
            shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
            MR_ComputeNodeInterfaceClient ComputerNode(protocol);
            //connect to node and call sort
            transport->open();
            if(task_iter->filenames.size() > 0)
              ComputerNode.Sort(task_iter->filenames.at(0), offset, size);
            transport->close();

            pthread_mutex_lock(&num_r_tasks_lock);
            num_redundant_tasks++;
            pthread_mutex_unlock(&num_r_tasks_lock);

            task_iter++;
            if(task_iter == tasks.end())
              break;
          } catch (...) {
            std::cout << "Couldn't (re)assign SORT task." << std::endl;
          }
        }
        iter++;
        if(iter == allServer.end())
          iter = allServer.begin();
      }
      //remove all reassigned tasks from dead node
    }
    tasks.clear();
  }

  //Checks to see if the Compute Node contains a merge task with the same unqiue task_number
  bool containsTask(std::vector<Task> tasks, int task_number) {
    if(tasks.size() == 0 && tasks.size() > 4000)
      return false;

    for(std::vector<Task>::iterator task_iter = tasks.begin(); task_iter != tasks.end(); task_iter++) {
      if(task_iter->merge_task_number == task_number)
        return true;
    }
    return false;
  }

  void reassignMergeTasks(std::vector<Task> &tasks, int redundancy) {
    bool task_exists = false;
    std::vector<Task>::iterator task_iter;
    unsigned int count = 0;
    merge_iter++;
    if(merge_iter == allServer.end())
      merge_iter = allServer.begin();

    for(int i = 0; i < redundancy; i++)
    {
      task_iter = tasks.begin();

      while(task_iter != tasks.end())
      {
        //don't reassign a completed task
        while((task_iter != tasks.end()) && task_iter->completed)
          task_iter++;
        if(task_iter == tasks.end()) //All tasks are completed
          break;

          task_exists = false;

          if(containsTask(merge_iter->second.tasks, task_iter->merge_task_number))
          {
            task_exists = true;
            break;
          }
          if(task_exists)
          {
            count++;
            if(count > total_alive_nodes)
            {
              count = 0;
              task_iter++;
            }
            else
            {
              merge_iter++;
              if(merge_iter == allServer.end())
               merge_iter = allServer.begin();
            }
            continue;
          }
          //Find an alive node that isn't performing this task
          if(!merge_iter->second.dead &&
             !containsTask(merge_iter->second.tasks, task_iter->merge_task_number))
          {
            try {
              Task merge_task_attr;
              merge_task_attr.type = MERGE;
              merge_task_attr.offset = 0; //<---- Not using for Merge
              merge_task_attr.size = 0; //<---- Not using for Merge
              merge_task_attr.filenames = task_iter->filenames;
              merge_task_attr.completed = false;
              merge_task_attr.merge_task_number = task_iter->merge_task_number;
              merge_iter->second.tasks.push_back(merge_task_attr);

              shared_ptr<TSocket> socket(new TSocket(merge_iter->first.ipAddress,
                                                     merge_iter->first.port));
              shared_ptr<TTransport> transport(new TBufferedTransport(socket));
              shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
              MR_ComputeNodeInterfaceClient ComputerNode(protocol);

              //connect to node and call merge
              transport->open();
              ComputerNode.Merge(task_iter->filenames, task_iter->merge_task_number);
              transport->close();

              //increment redundant tasks
              pthread_mutex_lock(&num_r_tasks_lock);
              num_redundant_tasks++;
              pthread_mutex_unlock(&num_r_tasks_lock);

              task_iter++;
              if(task_iter == tasks.end())
                break;
            } catch (...) {
              std::cout << "Couldn't (re)assign MERGE task." << std::endl;
            }
          }
        }
      }
      tasks.clear();
}

  bool sendFilename(const std::string& filename, const std::string& ipAddress, const int16_t port, const int64_t offset, const int64_t size) {
    bool alreadyExists = true;
    //remove the task associated with filename

    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      if((iter->first.ipAddress == ipAddress) &&
         (iter->first.port == port))
      {
        for(unsigned int i = 0; i < iter->second.tasks.size(); i++)
        {
          if((iter->second.tasks[i].offset == offset) &&
             (iter->second.tasks[i].size == size))
          {
            iter->second.tasks[i].completed = true;
            break;
          }
        }
        break;
      }
    }

    std::string tempFilename = filename.substr(0, filename.find("sorted") + 6); //get base name

    //if base filename already exists don't add it
    pthread_mutex_lock(&sort_lock);
    if (std::find(tempFilenames.begin(), tempFilenames.end(), tempFilename) == tempFilenames.end())
    {
      //std::cout << "FOUND" << std::endl;
      tempFilenames.push_back(tempFilename);
      filenames.push_back(filename);
      alreadyExists = false;
    }
    else
      num_redundant_tasks_killed++;
    pthread_mutex_unlock(&sort_lock);
    return alreadyExists;
  }

  bool callback_merge(const std::string& filename, const std::string& ipAddress, const int16_t port, const int64_t merge_task_number) {
    // Your implementation goes here
    bool alreadyExists = true;

    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      if((iter->first.ipAddress == ipAddress) &&
         (iter->first.port == port))
      {
        for(unsigned int i = 0; i < iter->second.tasks.size(); i++)
        {
          if((iter->second.tasks[i].merge_task_number == merge_task_number))
          {
            iter->second.tasks[i].completed = true;
            break;
          }
        }
        break;
      }
    }

    std::string tempFilename = filename.substr(0, filename.find("_", 25)+1);
    for(unsigned int i = filename.find("_", 25)+1; i < filename.length(); i++) {
      if(filename[i] == '_')
        break;
      else
        tempFilename += filename[i];
    }

    pthread_mutex_lock(&merge_lock);
    if (std::find(tempFilenames.begin(), tempFilenames.end(), tempFilename) == tempFilenames.end())
    {
      tempFilenames.push_back(tempFilename);
      filenames.push_back(filename);
      merge_files_left++;
      merge_expected_files--;
      total_files++;
      alreadyExists = false;
    } else if (filename.find("sorted") != std::string::npos) {
      alreadyExists = false;
    }
    pthread_mutex_unlock(&merge_lock);
    return alreadyExists;
  }

  void assignMergeTask(std::map<ServerInfo, Info>::iterator& map_iter) {
    std::vector<std::string> merge_task;
    Task merge_task_attr;

    unsigned int j = 0;
    pthread_mutex_lock(&q);
    //cycle through compute node list and assign tasks
      //for each compute node, get the first (merge_inter_files) filenames and construct a vector
      for(; j < (filenames.size() < merge_inter_files ? filenames.size() : merge_inter_files); j++) {
        merge_task.push_back(filenames[j]);
      }
      //These files will be deleted in Merge on Compute Node side
      filenames.erase(filenames.begin(), filenames.begin()+j);
    pthread_mutex_unlock(&q);
      if(map_iter == allServer.end()) {
        map_iter = allServer.begin(); //This sets iterator back to beginning of map so that we cycle through node list
      }

      //If all compute nodes are dead, then this will infinitely loop
      //NOTE: see NOTE below for infinite loop fix
      for(; map_iter != allServer.end();) {
        if(!map_iter->second.dead)
        {
          try {
            shared_ptr<TSocket> socket(new TSocket(map_iter->first.ipAddress, map_iter->first.port));
            shared_ptr<TTransport> transport(new TBufferedTransport(socket));
            shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
            MR_ComputeNodeInterfaceClient ComputerNode(protocol);

            merge_task_attr.type = MERGE;
            merge_task_attr.offset = 0; //<---- Not using for Merge
            merge_task_attr.size = 0; //<---- Not using for Merge
            merge_task_attr.merge_task_number = merge_task_number;
            merge_task_attr.filenames = merge_task;
            merge_task_attr.completed = false;
            map_iter->second.tasks.push_back(merge_task_attr);

            //connect to node and call merge
            transport->open();
            ComputerNode.Merge(merge_task, merge_task_number);
            transport->close();

            //increment redundant tasks
            pthread_mutex_lock(&num_r_tasks_lock);
            num_redundant_tasks++;
            pthread_mutex_unlock(&num_r_tasks_lock);

            merge_files_left-=merge_task.size();
            merge_expected_files++;

            merge_task_number++;
            map_iter++;
            break;
          } catch (...) {
            map_iter++;
            std::cout << "Couldn't assign original merge task. Trying another Compute Node." << std::endl;
          }
        } else if (++map_iter == allServer.end()) { //Start over from beginning of list
          map_iter = allServer.begin();
          //NOTE: could have a counter here.  Something like
          //count++;
          //if(count > expected number of merge tasks)
          //  ERROR all nodes are dead
          //the if argument above will never resolve to true if there is at least one node still alive
        } else {
          map_iter++;
        }
      }

      std::vector<Task> red_tasks;
      red_tasks.push_back(merge_task_attr);
      reassignMergeTasks(red_tasks, redundancy);
  }

  void getSorted(std::string& _return, const std::string& input_filename) {
    //Wait until my turn. Only one client job can be processed at a time
    pthread_mutex_lock(&m);
    //clear all tasks that have finished
    filenames.clear();
    merge_expected_files = 0;
    merge_task_number = 0;
    merge_files_left = 0;
    num_sort_files = 0;
    num_redundant_tasks = 0;
    num_faults = 0;
    num_redundant_tasks_killed = 0;
    tempFilenames.clear();

    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      iter->second.tasks.clear();
    }

    //Wake up all Compute Nodes which failed in the last task
    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      try {
        shared_ptr<TSocket> socket(new TSocket(iter->first.ipAddress, iter->first.port));
        shared_ptr<TTransport> transport(new TBufferedTransport(socket));
        shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
        MR_ComputeNodeInterfaceClient ComputerNode(protocol);

        //connect to node and call sort
        transport->open();
        ComputerNode.wakeup();
        transport->close();

        if(iter->second.dead)
          total_alive_nodes++;

        iter->second.dead = false;
      } catch (...) {
        std::cout << "Couldn't wakeup node since it was dead" << std::endl;
      }
    }

    std::ifstream fin;
    int offset = 0;
    int size;
    int filesize;
    std::vector<Task> redundant_tasks;

    fin.open(("../input_files/" + input_filename).c_str());
    if(fin.fail())
    {
      std::cerr << "Cannot open file " << input_filename << std::endl;
      _return = "";
      return;
    }

    //get filesize
    fin.seekg(0, std::ios_base::end);
    filesize = fin.tellg();

    //calculate needed tasks
    while(offset < filesize)
    {
      size = max_size;
      if(offset + size > filesize)
      {
        size = filesize - offset;
      }
      else
      {
        fin.seekg(offset + size, std::ios_base::beg);
        while((fin.peek() != ' ') && (fin.tellg() < filesize))
        {
          fin.seekg(-1, std::ios_base::cur);
          size -= 1;
        }
      }

      Task temp;
      temp.type = SORT;
      temp.offset = offset;
      temp.size = size;
      temp.completed = false;
      temp.filenames.push_back(input_filename);
      redundant_tasks.push_back(temp);

      num_sort_files++; //<---- Number of sort files I expect to get

      offset += size;
      if(offset >= filesize)
        break;
    }

    //send off tasks
    reassignSortTasks(redundant_tasks, redundancy + 1);

    //Don't forget to do a conditional wait.
    while(filenames.size() != num_sort_files);
    //clear all tasks that have finished
    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      iter->second.tasks.clear();
    }

    merge_files_left = filenames.size(); //Number of files that need to be merged
    std::map<ServerInfo, Info>::iterator map_iter = allServer.begin();

    sleep(1); //Wait for all sort tasks to finish

    //Number of merge tasks for just first round
    int num_tasks = (merge_files_left%merge_inter_files > 0 ? ((merge_files_left/merge_inter_files)+1) :
                                                             (merge_files_left/merge_inter_files));
    for(int i = 0; i < num_tasks; i++) {
      pthread_mutex_lock(&merge_lock);
      assignMergeTask(map_iter);
      pthread_mutex_unlock(&merge_lock);
    }
    //2nd round of merges and beyond (3rd, 4th, 5th, etc..)
    //Stopping condition (merge_expected_files == 0, merge_files_left == 1)
    while(true) {
      while(merge_expected_files != 0); //Wait for all merge tasks to be done in previous round
      if(merge_files_left == 1) { //If the numbe rof files left is 1, then we are done
        break;
      } else {
        assignMergeTask(map_iter); //Otherwise, create the merge tasks for the next round
      }
    }

    //Write result to output folder
    std::stringstream fname;
    std::string inter_output_filename = filenames[0];
    std::string final_output_filename;
    fname << "../output_files/final_" << request_number;
    request_number++;
    final_output_filename = fname.str();

    std::ifstream srce(inter_output_filename.c_str(), std::ios::binary);
    std::ofstream dest(final_output_filename.c_str(), std::ios::binary);
    dest << srce.rdbuf();
    remove(inter_output_filename.c_str());

    _return = final_output_filename.substr(1, final_output_filename.size()-1);
    system("exec rm -r ../intermediary_files/*");

    //tell compute nodes to output statistics
    for(std::map<ServerInfo, Info>::iterator iter = allServer.begin();
        iter != allServer.end(); iter++)
    {
      try {
        shared_ptr<TSocket> socket(new TSocket(iter->first.ipAddress, iter->first.port));
        shared_ptr<TTransport> transport(new TBufferedTransport(socket));
        shared_ptr<TProtocol> protocol(new TBinaryProtocol(transport));
        MR_ComputeNodeInterfaceClient ComputerNode(protocol);

        //connect to node and call outputStats()
        transport->open();
        ComputerNode.outputStats();
        transport->close();
      } catch (...) {
      }
    }

    std::cout << "Number of faults: " << num_faults << std::endl
              << "Number of redundant tasks: " << num_redundant_tasks << std::endl
              << "Number of redundant tasks killed: " << num_redundant_tasks_killed << std::endl;

    printf("getSorted\n");
    pthread_mutex_unlock(&m);
  }

};

void* heartbeatTimerWrapper(void* useless)
{
  ((MR_ServerInterfaceHandler*)ServerHandle)->heartbeatTimer();
  return useless;
}

/*Returns IP Address of local machine*/
std::string getIPAddress() {
  struct ifaddrs * ifAddrStruct = NULL;
  struct ifaddrs * ifa = NULL;
  void * tmpAddrPtr = NULL;
  std::string IPAddress = "";

  getifaddrs(&ifAddrStruct);

  for(ifa = ifAddrStruct; ifa != NULL; ifa = ifa->ifa_next) {
    if(!ifa->ifa_addr) {
      continue;
    }

    if(ifa->ifa_addr->sa_family == AF_INET) {
      tmpAddrPtr = &((struct sockaddr_in *)ifa->ifa_addr)->sin_addr;
      char addressBuffer[INET_ADDRSTRLEN];
      inet_ntop(AF_INET, tmpAddrPtr, addressBuffer, INET_ADDRSTRLEN);
      IPAddress = std::string(addressBuffer);
    }
  }

  if(ifAddrStruct != NULL)
    freeifaddrs(ifAddrStruct);

  return IPAddress;
}


int main(int argc, char **argv) {

  if(argc != 5) {
    std::cout << "Usage: ./server <port> <chunk_size> <num_merge_files> <num_redundant_tasks>"
    << std::endl;
    return 1;
  }

  int my_port = atoi(argv[1]);
  int chunk_size = atoi(argv[2]);
  int merge_num_files = atoi(argv[3]);
  int redundant_tasks = atoi(argv[4]);
  std::cout << "Here is my IP Address: " << getIPAddress() << std::endl;
  shared_ptr<MR_ServerInterfaceHandler> handler(new MR_ServerInterfaceHandler(chunk_size, merge_num_files, redundant_tasks));
  shared_ptr<TProcessor> processor(new MR_ServerInterfaceProcessor(handler));
  shared_ptr<TServerTransport> serverTransport(new TServerSocket(my_port));
  shared_ptr<TTransportFactory> transportFactory(new TBufferedTransportFactory());
  shared_ptr<TProtocolFactory> protocolFactory(new TBinaryProtocolFactory());

  TThreadedServer server(processor, serverTransport, transportFactory, protocolFactory);
  server.serve();
  return 0;
}
